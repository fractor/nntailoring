# Gerald's stupid table
# R(m,n) = R(m - 1, n) + R(m - 1 , n - 1)

import numpy as np 
import matplotlib.pyplot as plt
import sys
sys.setrecursionlimit(1500)

gerald_file = open('tnk.csv' , 'w')

def R(m , n):
  if values[n][m] >= 0:
    return values[n][m]
  elif n == 0:
    values[n][m] = 0
    return 0
  elif m == 1 and n >=1:
    values[n][m] = 2
    return 2
  else:
    values[n][m] = R(m - 1, n) + R(m - 1, n - 1)
    return values[n][m]

values = []
n_max = 150
m_max = 3 * n_max
col_labels = []
for n in range(n_max + 1):
  values.append([])
  col_labels.append("{}".format(n))
  for m in range(m_max + 1):
    values[n].append(-1)

# R(m_max, n_max)
# for i in range(n_max + 1):
#   R(m_max , n_max - i)

for m in range(1 , m_max + 1):
  for n in range(n_max + 1):
    R(m, n)

# temp_str = "m/n"
# for item in col_labels:
#   temp_str += ",{}".format(item)
# temp_str+= "\n"
# gerald_file.write(temp_str)
for m in range(1 , m_max + 1):
  temp_str = "" #"{}".format(m)
  for n in range(n_max + 1):
    temp_str += "{},".format(values[n][m])
  temp_str += "\n"
  gerald_file.write(temp_str)


"""
#fig, ax =plt.figure(1)
data = np.array(values)
#clust_data = np.random.random((10,3))
collabel=(col_labels)
#ax.axis('tight')
#ax.axis('off')

fig = plt.figure(1)
#plt.subplots_adjust(left=0.2, top=0.8)
fig.subplots_adjust(left=0.2,top=0.8, wspace=5)
ax = plt.subplot2grid((4,3), (0,0), colspan=2, rowspan=2)
ax.axis("off") 
#ax.yaxis.set_visible(False)
the_table = ax.table(cellText=data,colLabels=collabel,loc='bottom')
fig.set_size_inches(w=6, h=5)

ax.plot(data[:,0])
plt.show()
"""


gerald_file.close()

data = np.zeros((n_max,m_max))
for n in range(n_max):
  data[n] = values[n+1][1:]
for m in range(m_max):
  data[:,m] *= 2.0**(-m-1)


curves=[]

#for n in [9,19,29,39,49,59,69,79,89,99]:
for n in [1,3,7,15,31,63,127]:
  points = []
  index = 0.0
  for value in data[n]:
    index += 1.0
    points.append((index/(n+1), value))
    if index/(n+1) >= 3:
      break
  curves.append(points)
  plt.plot([i for i,j in points], [j for i,j in points], label=n+1)
plt.xticks([0,1,2])
plt.yticks([0,0.5,1])
plt.grid(linestyle="dotted")
plt.legend(title="Input dimension k")
plt.xlabel('Number of samples n divided by k')
plt.ylabel('Perfect classification accuracy T(n,k)/2**n')
plt.show()

empiric_2 = dict()

empiric_2[1] = [1.0, 1.0, 1.0, 0.875, 0.6875, 0.5, 0.34375, 0.2265625, 0.14453125, 0.08984375, 0.068359375, 0.021484375, 0.013671875, 0.01171875, 0.009765625]

empiric_2[2] = [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.953125, 0.796875, 0.7109375, 0.56640625, 0.416015625, 0.375, 0.232421875, 0.150390625, 0.125, 0.09765625, 0.060546875, 0.033203125, 0.01953125, 0.01953125, 0.005859375, 0.00390625, 0.001953125, 0.0]

empiric_2[3] = [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.96875, 0.953125, 0.935546875, 0.85546875, 0.830078125, 0.73828125, 0.646484375, 0.53515625, 0.4765625, 0.400390625, 0.359375, 0.265625, 0.212890625, 0.16015625, 0.115234375, 0.08984375, 0.05859375, 0.048828125, 0.033203125, 0.017578125, 0.01171875, 0.009765625, 0.001953125, 0.00390625, 0.005859375, 0.0]

empiric_2[4] = [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.9921875, 0.98828125, 0.974609375, 0.97265625, 0.96484375, 0.931640625, 0.900390625, 0.89453125, 0.767578125, 0.720703125, 0.671875, 0.615234375, 0.54296875, 0.470703125, 0.4140625, 0.31640625, 0.244140625, 0.21875, 0.197265625, 0.1328125, 0.09765625, 0.048828125, 0.052734375, 0.03515625, 0.03125, 0.03125, 0.00390625, 0.009765625, 0.001953125, 0.00390625, 0.0]

empiric_2[5] = [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.99609375, 0.99609375, 0.982421875, 0.982421875, 0.953125, 0.916015625, 0.908203125, 0.853515625, 0.79296875, 0.755859375, 0.724609375, 0.66015625, 0.61328125, 0.552734375, 0.5234375, 0.431640625, 0.37890625, 0.251953125, 0.201171875, 0.193359375, 0.126953125, 0.09765625, 0.078125, 0.056640625, 0.033203125, 0.03515625, 0.03515625, 0.009765625, 0.009765625, 0.015625, 0.0078125, 0.001953125, 0.009765625, 0.0]

empiric_2[6] = [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.998046875, 0.994140625, 0.990234375, 0.986328125, 0.978515625, 0.9765625, 0.95703125, 0.94140625, 0.923828125, 0.91796875, 0.83984375, 0.814453125, 0.78125, 0.740234375, 0.65234375, 0.5859375, 0.46875, 0.408203125, 0.314453125, 0.2421875, 0.24609375, 0.16015625, 0.123046875, 0.107421875, 0.09765625, 0.0703125, 0.044921875, 0.01953125, 0.033203125, 0.02734375, 0.01171875, 0.017578125, 0.0078125, 0.005859375, 0.009765625, 0.005859375, 0.005859375, 0.001953125, 0.00390625, 0.0]


empiric_2[7] = [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.984375, 0.98828125, 0.990234375, 0.9921875, 0.98046875, 0.955078125, 0.9609375, 0.91796875, 0.869140625, 0.8515625, 0.86328125, 0.794921875, 0.796875, 0.775390625, 0.7578125, 0.72265625, 0.640625, 0.595703125, 0.541015625, 0.5234375, 0.4609375, 0.328125, 0.287109375, 0.197265625, 0.17578125, 0.1484375, 0.12890625, 0.107421875, 0.0625, 0.037109375, 0.04296875, 0.041015625, 0.046875, 0.015625, 0.009765625, 0.0078125, 0.009765625, 0.005859375, 0.0078125, 0.00390625, 0.00390625, 0.0, 0.0, 0.0, 0.0, 0.001953125, 0.001953125, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]

empiric_2[8] = [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.994140625, 0.998046875, 0.998046875, 0.99609375, 0.98828125, 0.994140625, 0.984375, 0.986328125, 0.970703125, 0.958984375, 0.96484375, 0.8828125, 0.84765625, 0.802734375, 0.677734375, 0.63671875, 0.609375, 0.544921875, 0.470703125, 0.396484375, 0.349609375, 0.294921875, 0.283203125, 0.1953125, 0.158203125, 0.16015625, 0.126953125, 0.115234375, 0.076171875, 0.064453125, 0.0625, 0.03515625, 0.029296875, 0.015625, 0.015625, 0.013671875, 0.009765625, 0.009765625, 0.009765625, 0.009765625, 0.00390625, 0.0]

empiric_3 = dict()

empiric_3[1] = [1.0, 1.0, 1.0, 1.0, 0.9375, 0.8125, 0.65625, 0.5, 0.36328125, 0.25390625, 0.171875, 0.119140625, 0.07421875, 0.048828125, 0.015625, 0.015625, 0.013671875, 0.005859375]

empiric_3[2] = [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.9921875, 0.953125, 0.9453125, 0.861328125, 0.80078125, 0.671875, 0.6015625, 0.5, 0.419921875, 0.33984375, 0.296875, 0.19921875, 0.162109375, 0.1015625, 0.0546875, 0.0625, 0.0390625, 0.01953125, 0.009765625, 0.013671875, 0.00390625, 0.001953125, 0.001953125, 0.0]

empiric_3[3] = [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.99609375, 1.0, 0.99609375, 0.99609375, 0.98046875, 0.96875, 0.955078125, 0.908203125, 0.892578125, 0.853515625, 0.77734375, 0.66796875, 0.5859375, 0.53125, 0.455078125, 0.361328125, 0.2734375, 0.26171875, 0.1953125, 0.16015625, 0.10546875, 0.09765625, 0.060546875, 0.05078125, 0.029296875, 0.013671875, 0.01171875, 0.017578125, 0.01171875, 0.0078125, 0.00390625, 0.00390625, 0.0078125, 0.0]

empiric_3[4] = [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.998046875, 1.0, 0.9921875, 0.986328125, 0.978515625, 0.95703125, 0.935546875, 0.8984375, 0.861328125, 0.77734375, 0.68359375, 0.69921875, 0.611328125, 0.51953125, 0.484375, 0.40625, 0.322265625, 0.2890625, 0.275390625, 0.189453125, 0.162109375, 0.12890625, 0.068359375, 0.072265625, 0.048828125, 0.03515625, 0.044921875, 0.029296875, 0.01953125, 0.00390625, 0.009765625, 0.005859375, 0.00390625, 0.0078125, 0.0]

empiric_3[5] = [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.994140625, 1.0, 0.9921875, 0.978515625, 0.982421875, 0.97265625, 0.9375, 0.923828125, 0.880859375, 0.83203125, 0.78125, 0.734375, 0.681640625, 0.59765625, 0.5546875, 0.484375, 0.40234375, 0.396484375, 0.3515625, 0.255859375, 0.21484375, 0.19140625, 0.140625, 0.150390625, 0.10546875, 0.068359375, 0.046875, 0.05859375, 0.025390625, 0.029296875, 0.041015625, 0.015625, 0.009765625, 0.01171875, 0.009765625, 0.001953125, 0.0]

empiric_3[6] = [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.99609375, 0.984375, 0.982421875, 0.96484375, 0.96484375, 0.955078125, 0.916015625, 0.888671875, 0.83203125, 0.814453125, 0.765625, 0.693359375, 0.685546875, 0.59765625, 0.564453125, 0.5, 0.4296875, 0.3203125, 0.3046875, 0.3125, 0.240234375, 0.228515625, 0.19140625, 0.173828125, 0.115234375, 0.09375, 0.0625, 0.052734375, 0.044921875, 0.052734375, 0.025390625, 0.02734375, 0.029296875, 0.015625, 0.0078125, 0.001953125, 0.005859375, 0.001953125, 0.00390625, 0.0078125, 0.0]


empiric_3[7] = [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.998046875, 1.0, 0.99609375, 0.9921875, 0.990234375, 0.9765625, 0.978515625, 0.970703125, 0.94140625, 0.92578125, 0.908203125, 0.884765625, 0.830078125, 0.78515625, 0.744140625, 0.728515625, 0.646484375, 0.578125, 0.572265625, 0.5, 0.443359375, 0.40625, 0.3359375, 0.30078125, 0.265625, 0.212890625, 0.16015625, 0.1640625, 0.15234375, 0.09765625, 0.05078125, 0.044921875, 0.048828125, 0.03515625, 0.037109375, 0.029296875, 0.017578125, 0.01953125, 0.017578125, 0.00390625, 0.005859375, 0.00390625, 0.0]

empiric_3[8] = [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.99609375, 0.994140625, 0.994140625, 0.99609375, 0.9921875, 0.96875, 0.962890625, 0.943359375, 0.919921875, 0.92578125, 0.912109375, 0.859375, 0.814453125, 0.849609375, 0.74609375, 0.69140625, 0.619140625, 0.623046875, 0.51953125, 0.494140625, 0.458984375, 0.42578125, 0.365234375, 0.275390625, 0.28125, 0.2109375, 0.173828125, 0.146484375, 0.11328125, 0.095703125, 0.08984375, 0.052734375, 0.072265625, 0.056640625, 0.029296875, 0.04296875, 0.041015625, 0.0234375, 0.0234375, 0.017578125, 0.017578125, 0.015625, 0.00390625, 0.005859375, 0.005859375, 0.00390625, 0.00390625, 0.00390625, 0.0]


empiric_4 = dict()

empiric_4[1] = [1.0, 1.0, 1.0, 1.0, 1.0, 0.96875, 0.875, 0.765625, 0.59375, 0.46484375, 0.38671875, 0.251953125, 0.19921875, 0.12890625, 0.09375, 0.0625, 0.041015625, 0.025390625, 0.0078125, 0.0078125, 0.005859375]

empiric_4[2] = [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.953125, 0.9453125, 0.88671875, 0.9140625, 0.849609375, 0.76953125, 0.7421875, 0.65234375, 0.607421875, 0.55859375, 0.455078125, 0.41796875, 0.337890625, 0.251953125, 0.216796875, 0.166015625, 0.11328125, 0.119140625, 0.087890625, 0.068359375, 0.05078125, 0.029296875, 0.0234375, 0.015625, 0.01171875, 0.0, 0.00390625, 0.005859375, 0.001953125, 0.0, 0.0, 0.001953125, 0.001953125]

empiric_4[3] = [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.99609375, 0.99609375, 0.990234375, 0.982421875, 0.978515625, 0.96875, 0.947265625, 0.92578125, 0.931640625, 0.845703125, 0.806640625, 0.833984375, 0.720703125, 0.673828125, 0.578125, 0.51953125, 0.53125, 0.4609375, 0.37890625, 0.310546875, 0.259765625, 0.21875, 0.169921875, 0.140625, 0.1171875, 0.111328125, 0.044921875, 0.060546875, 0.05078125, 0.02734375, 0.03125, 0.021484375, 0.013671875, 0.015625, 0.00390625, 0.005859375, 0.001953125, 0.001953125, 0.001953125, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]

empiric_4[4] = [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.99609375, 1.0, 0.99609375, 0.9921875, 0.994140625, 0.984375, 0.986328125, 0.96875, 0.9765625, 0.9765625, 0.939453125, 0.921875, 0.904296875, 0.87890625, 0.845703125, 0.771484375, 0.734375, 0.6796875, 0.658203125, 0.560546875, 0.50390625, 0.5, 0.421875, 0.365234375, 0.283203125, 0.314453125, 0.25, 0.25, 0.189453125, 0.15234375, 0.1328125, 0.09765625, 0.072265625, 0.0625, 0.044921875, 0.0546875, 0.044921875, 0.017578125, 0.02734375, 0.01171875, 0.015625, 0.0078125, 0.0078125, 0.005859375, 0.00390625, 0.005859375, 0.00390625, 0.0, 0.0, 0.0, 0.001953125, 0.0, 0.0, 0.001953125, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]

empiric_4[5] = [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.998046875, 1.0, 0.994140625, 0.9921875, 0.9921875, 0.984375, 0.96484375, 0.958984375, 0.9453125, 0.91015625, 0.916015625, 0.904296875, 0.869140625, 0.859375, 0.833984375, 0.759765625, 0.76953125, 0.6875, 0.638671875, 0.609375, 0.57421875, 0.478515625, 0.45703125, 0.421875, 0.40234375, 0.314453125, 0.263671875, 0.291015625, 0.27734375, 0.1953125, 0.1484375, 0.140625, 0.10546875, 0.087890625, 0.07421875, 0.0625, 0.041015625, 0.044921875, 0.02734375, 0.037109375, 0.03515625, 0.025390625, 0.01171875, 0.009765625, 0.0078125, 0.00390625, 0.015625, 0.0078125, 0.005859375, 0.00390625, 0.0, 0.0, 0.001953125, 0.001953125, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.001953125, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]

empiric_4[6] = [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.994140625, 0.99609375, 0.998046875, 0.99609375, 0.99609375, 0.994140625, 0.98828125, 0.97265625, 0.962890625, 0.96484375, 0.943359375, 0.958984375, 0.943359375, 0.908203125, 0.884765625, 0.875, 0.802734375, 0.78125, 0.779296875, 0.712890625, 0.68359375, 0.69921875, 0.63671875, 0.55859375, 0.53125, 0.46875, 0.447265625, 0.443359375, 0.33203125, 0.306640625, 0.28125, 0.26953125, 0.220703125, 0.2265625, 0.1875, 0.15625, 0.12109375, 0.091796875, 0.080078125, 0.068359375, 0.0546875, 0.044921875, 0.033203125, 0.03515625, 0.015625, 0.009765625, 0.009765625, 0.017578125, 0.009765625, 0.0078125, 0.015625, 0.0078125, 0.001953125, 0.001953125, 0.005859375, 0.0, 0.001953125, 0.0, 0.001953125, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]

empiric_4[7] = [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.9921875, 0.99609375, 1.0, 0.9921875, 1.0, 0.990234375, 0.9921875, 0.9765625, 0.9765625, 0.970703125, 0.974609375, 0.958984375, 0.9453125, 0.96484375, 0.91796875, 0.916015625, 0.853515625, 0.865234375, 0.80859375, 0.77734375, 0.802734375, 0.7421875, 0.693359375, 0.68359375, 0.615234375, 0.5859375, 0.548828125, 0.498046875, 0.421875, 0.388671875, 0.384765625, 0.34765625, 0.33203125, 0.24609375, 0.20703125, 0.259765625, 0.1875, 0.1640625, 0.154296875, 0.107421875, 0.09765625, 0.115234375, 0.072265625, 0.083984375, 0.052734375, 0.0546875, 0.037109375, 0.02734375, 0.01953125, 0.01953125, 0.0234375, 0.01171875, 0.0078125, 0.017578125, 0.00390625, 0.005859375, 0.005859375, 0.005859375, 0.00390625, 0.001953125, 0.00390625, 0.001953125, 0.001953125, 0.001953125, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]

empiric_4[8] = [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.998046875, 1.0, 1.0, 0.994140625, 0.998046875, 0.9921875, 0.994140625, 0.998046875, 0.98828125, 0.994140625, 0.99609375, 0.98046875, 0.970703125, 0.951171875, 0.95703125, 0.935546875, 0.955078125, 0.9140625, 0.892578125, 0.849609375, 0.859375, 0.845703125, 0.7890625, 0.75390625, 0.744140625, 0.669921875, 0.6640625, 0.626953125, 0.59765625, 0.6015625, 0.501953125, 0.49609375, 0.427734375, 0.404296875, 0.3671875, 0.3671875, 0.30859375, 0.283203125, 0.259765625, 0.205078125, 0.173828125, 0.208984375, 0.1484375, 0.109375, 0.12109375, 0.12109375, 0.099609375, 0.0625, 0.048828125, 0.041015625, 0.03515625, 0.041015625, 0.0390625, 0.01953125, 0.02734375, 0.017578125, 0.01171875, 0.0078125, 0.01171875, 0.009765625, 0.0, 0.0078125, 0.00390625, 0.00390625, 0.0, 0.0, 0.001953125, 0.001953125, 0.001953125, 0.0, 0.001953125, 0.001953125, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]

for h in [1,2,3,4,5,6,7,8]:
  points = []
  k = 2
  index = 0.0
  for value in empiric_2[h]:
    index += 1.0
    points.append((index/(h*(k+2)+1), value))
    if index/(h*(k+2)+1) >= 3:
      break
  curves.append(points)
  plt.plot([i for i,j in points], [j for i,j in points], label=h)
plt.xticks([0,1,2])
plt.yticks([0,0.5,1])
plt.grid(linestyle="dotted")
plt.legend(title="Number of hidden nodes h")
plt.xlabel('Number of samples n divided by number of weights k')
plt.ylabel('Perfect classification accuracy/2**n')
plt.title('Input dimension:%d'%k)
plt.show()

for h in [1,2,3,4,5,6,7,8]:
  points = []
  k = 3
  index = 0.0
  for value in empiric_3[h]:
    index += 1.0
    points.append((index/(h*(k+2)+1), value))
    if index/(h*(k+2)+1) >= 3:
      break
  curves.append(points)
  plt.plot([i for i,j in points], [j for i,j in points], label=h)
plt.xticks([0,1,2])
plt.yticks([0,0.5,1])
plt.grid()
plt.legend(title="Number of hidden nodes h")
plt.xlabel('Number of samples n divided by number of weights k')
plt.ylabel('Perfect classification accuracy/2**n')
plt.title('Input dimension:%d'%k)
plt.show()

for h in [1,2,3,4,5,6,7,8]:
  points = []
  k = 4
  index = 0.0
  for value in empiric_4[h]:
    index += 1.0
    points.append((index/(h*(k+2)+1), value))
    if index/(h*(k+2)+1) >= 3:
      break
  curves.append(points)
  plt.plot([i for i,j in points], [j for i,j in points], label=h)
plt.xticks([0,1,2])
plt.yticks([0,0.5,1])
plt.grid()
plt.legend(title="Number of hidden nodes h")
plt.xlabel('Number of samples n divided by number of weights k')
plt.ylabel('Perfect classification accuracy/2**n')
plt.title('Input dimension:%d'%k)
plt.show()



for h in [1,2,3,4,5,6,7,8]:
  points = []
  k = 2
  index = 0.0
  for value in empiric_2[h]:
    index += 1.0
    points.append((index/(h*(k+1)), value))
    if index/(h*(k+1)) >= 3:
      break
  curves.append(points)
  plt.plot([i for i,j in points], [j for i,j in points], label=h)
plt.xticks([0,1,2])
plt.yticks([0,0.5,1])
plt.grid()
plt.legend(title="Number of hidden nodes h")
plt.xlabel('Number of samples n divided by number of weights k')
plt.ylabel('Perfect classification accuracy/2**n')
plt.title('Input dimension:%d'%k)
plt.show()

for h in [1,2,3,4,5,6,7,8]:
  points = []
  k = 3
  index = 0.0
  for value in empiric_3[h]:
    index += 1.0
    points.append((index/(h*(k+1)), value))
    if index/(h*(k+1)) >= 3:
      break
  curves.append(points)
  plt.plot([i for i,j in points], [j for i,j in points], label=h)
plt.xticks([0,1,2])
plt.yticks([0,0.5,1])
plt.grid()
plt.legend(title="Number of hidden nodes h")
plt.xlabel('Number of samples n divided by number of weights k')
plt.ylabel('Perfect classification accuracy/2**n')
plt.title('Input dimension:%d'%k)
plt.show()

for h in [1,2,3,4,5,6,7,8]:
  points = []
  k = 4
  index = 0.0
  for value in empiric_4[h]:
    index += 1.0
    points.append((index/(h*(k+1)), value))
    if index/(h*(k+1)) >= 3:
      break
  curves.append(points)
  plt.plot([i for i,j in points], [j for i,j in points], label=h)
plt.xticks([0,1,2])
plt.yticks([0,0.5,1])
plt.grid()
plt.legend(title="Number of hidden nodes h")
plt.xlabel('Number of samples n divided by number of weights k')
plt.ylabel('Perfect classification accuracy/2**n')
plt.title('Input dimension:%d'%k)
plt.show()


